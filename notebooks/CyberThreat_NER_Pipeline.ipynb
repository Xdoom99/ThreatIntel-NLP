!pip install -U spacy
!python -m spacy download en_core_web_sm
!pip install -U transformers

threat_text = """
On July 25, 2024, APT29 initiated a new malware campaign using a modified version of Cobalt Strike.
The malware connects to 198.51.100.23 and downloads additional payloads.
Indicators include SHA256: d41d8cd98f00b204e9800998ecf8427e and domains like badsite.ru.
The attack exploited CVE-2024-12345, a critical RCE vulnerability in Outlook.
"""

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(threat_text)

print("ðŸ“ Named Entities Found:")
for ent in doc.ents:
    print(f"{ent.text} â†’ {ent.label_}")

import re

def extract_custom_indicators(text):
    ip_pattern = r"\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b"
    hash_pattern = r"\b[a-fA-F0-9]{32,64}\b"
    cve_pattern = r"CVE-\d{4}-\d{4,7}"
    domain_pattern = r"\b(?:[\w-]+\.)+[a-z]{2,}\b"

    return {
        "IP_ADDRESS": re.findall(ip_pattern, text),
        "HASH": re.findall(hash_pattern, text),
        "CVE": re.findall(cve_pattern, text),
        "DOMAIN": re.findall(domain_pattern, text)
    }

custom_entities = extract_custom_indicators(threat_text)

for label, matches in custom_entities.items():
    print(f"ðŸ” {label}: {matches}")

print("ðŸ“¦ Final Entity Extraction:\n")

# SpaCy Entities
for ent in doc.ents:
    print(f"{ent.text} â†’ {ent.label_}")

# Custom Entities
for label, matches in custom_entities.items():
    for match in matches:
        print(f"{match} â†’ {label}")

!pip install feedparser newspaper3k

!pip install newspaper3k --upgrade
!pip install lxml[html_clean]

import feedparser
from newspaper import Article

def get_articles_from_rss(rss_url, limit=3):
    feed = feedparser.parse(rss_url)
    articles = []

    for entry in feed.entries[:limit]:
        try:
            a = Article(entry.link)
            a.download()
            a.parse()
            articles.append({
                "title": entry.title,
                "link": entry.link,
                "text": a.text
            })
        except Exception as e:
            print(f"âŒ Error fetching article: {entry.link}\n{e}")

    return articles

rss_url = "https://threatpost.com/feed/"
articles = get_articles_from_rss(rss_url, limit=2)  # Increase limit if needed

for article in articles:
    print(f"\nðŸ“° {article['title']}")
    print(f"ðŸ”— {article['link']}\n")

    text = article['text']
    doc = nlp(text)
    custom_entities = extract_custom_indicators(text)

    print("ðŸ“¦ Final Entity Extraction:\n")
    for ent in doc.ents:
        print(f"{ent.text} â†’ {ent.label_}")
    for label, matches in custom_entities.items():
        for match in matches:
            print(f"{match} â†’ {label}")

!pip install openai

!pip install transformers

from transformers import pipeline

# Load the summarization model (can take a few seconds the first time)
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

def summarize_article(text):
    # Hugging Face models can have a max token limit â€” trim long text
    trimmed_text = text[:1024]

    summary = summarizer(trimmed_text, max_length=130, min_length=30, do_sample=False)
    return summary[0]['summary_text']

for article in articles:
    print(f"\nðŸ“° {article['title']}")
    print(f"ðŸ”— {article['link']}\n")

    text = article['text']
    doc = nlp(text)
    custom_entities = extract_custom_indicators(text)

    print("ðŸ“¦ Entity Extraction:")
    for ent in doc.ents:
        print(f"{ent.text} â†’ {ent.label_}")
    for label, matches in custom_entities.items():
        for match in matches:
            print(f"{match} â†’ {label}")

    print("\nðŸ“ Auto Summary:")
    try:
        summary = summarize_article(text)
        print(summary)
    except Exception as e:
        print(f"âŒ Summary error: {e}")

from google.colab import drive
drive.mount('/content/drive')

import os

def export_to_markdown(article, entities, summary, folder_path="/content/drive/MyDrive/CyberNERReports"):
    import os  # add this at the top of the cell if it's not already there

    title = article['title']
    link = article['link']
    filename = f"{title[:50].replace(' ', '_').replace('/', '_')}.md"
    filepath = os.path.join(folder_path, filename)

    # âœ… Create the folder if it doesn't exist
    os.makedirs(folder_path, exist_ok=True)

    with open(filepath, 'w') as f:
        f.write(f"# {title}\n\n")
        f.write(f"ðŸ”— [Source Link]({link})\n\n")

        f.write("## ðŸ“¦ Extracted Entities\n")
        for label, values in entities.items():
            if values:
                f.write(f"**{label}**: {', '.join(set(values))}\n\n")

        f.write("## ðŸ“ Auto Summary\n")
        f.write(summary.strip() + "\n")

    print(f"âœ… Saved report: {filepath}")


from collections import defaultdict

for article in articles:
    print(f"\nðŸ“° {article['title']}")
    print(f"ðŸ”— {article['link']}\n")

    text = article['text']
    doc = nlp(text)
    custom_entities = extract_custom_indicators(text)

    # Combine SpaCy + Custom Entities
    all_entities = defaultdict(list)
    for ent in doc.ents:
        all_entities[ent.label_].append(ent.text)
    for label, matches in custom_entities.items():
        all_entities[label].extend(matches)

    print("ðŸ“¦ Entity Extraction:")
    for label, items in all_entities.items():
        print(f"{label}: {set(items)}")

    print("\nðŸ“ Auto Summary:")
    try:
        summary = summarize_article(text)
        print(summary)
    except Exception as e:
        summary = f"âŒ Summary error: {e}"
        print(summary)

    # Export report
    export_to_markdown(article, all_entities, summary)

!pip freeze > requirements.txt

